{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    'max_depth':(1,50),\n",
    "    'n_estimators': (1,150)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(max_depth, \n",
    "                n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    tree_model = DecisionTreeClassifier()\n",
    "    model = BaggingClassifier(base_estimator = tree_model, n_estimators=int(n_estimators))\n",
    "    model.fit(trainx,trainy)\n",
    "    y_pred=model.predict(valx)\n",
    "    acc =accuracy_score(y_pred,valy)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(max_depth, \n",
    "                n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    tree_model = DecisionTreeClassifier(max_depth=int(max_depth))\n",
    "    model = AdaBoostClassifier(tree_model, n_estimators=int(n_estimators))\n",
    "    model.fit(trainx,trainy)\n",
    "    y_pred=model.predict(valx)\n",
    "    acc =accuracy_score(y_pred,valy)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    \n",
    "    train_x = train.iloc[:, 1:785]\n",
    "    train_y = train[train.keys()[0]]\n",
    "    test_x = test.iloc[:, 1:785]\n",
    "    test_y = test[test.keys()[0]]\n",
    "    \n",
    "    #splitting data into training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.1, random_state=1)\n",
    "    x_train1, x_train2, y_train1, y_train2 = train_test_split(train_x, train_y, test_size=0.6, random_state=1)\n",
    "    \n",
    "    #Final data\n",
    "    '''\n",
    "    X_val, y_val => validation data\n",
    "    x_train1, y_train1 => training data\n",
    "    '''\n",
    "   \n",
    "    ### Normalization\n",
    "    #x_train, x_test = train_x.float(), test_x.float()\n",
    "    train_mean,train_std = x_train1.mean(),x_train1.std()\n",
    "    train_mean,train_std\n",
    "    \n",
    "    x_train = normalize(x_train1, train_mean, train_std)\n",
    "    x_val = normalize(X_val, train_mean, train_std)\n",
    "    \n",
    "    return x_train, y_train1, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Loading data\n",
    "    trainx,trainy,valx,valy = read()\n",
    "    #Initiating optimization for bagging\n",
    "    optimizer = BayesianOptimization(\n",
    "    f=bagging,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    "    )\n",
    "    optimizer.maximize(n_iter=50)\n",
    "    #Appending Accuracy\n",
    "    bagging_accuracy = []\n",
    "    for i, res in enumerate(optimizer.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "        bagging_accuracy.append(optimizer.res[i]['target'])\n",
    "    #Plotting iteration Vs performance\n",
    "    plt.plot(np.arange(0,55),bagging_accuracy, label = 'Bagging Accuracy')\n",
    "    plt.xlabel(\"BO iterations\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.title(\"Bayesian Optimization Performance over Bagging\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('plot2ia.png')\n",
    "    #Initiating optimization for boosting\n",
    "    optimizer1 = BayesianOptimization(\n",
    "    f=boosting,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    "    )\n",
    "    optimizer1.maximize(n_iter=50)\n",
    "    #Initiating optimization for boosting\n",
    "    boosting_accuracy = []\n",
    "    for i, res in enumerate(optimizer1.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "        boosting_accuracy.append(optimizer1.res[i]['target'])\n",
    "    #Plotting\n",
    "    plt.plot(np.arange(0,55),boosting_accuracy, label = 'Boosting Accuracy')\n",
    "    plt.xlabel(\"BO iterations\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.title(\"Bayesian Optimization Performance over Boosting\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('plot2b.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
